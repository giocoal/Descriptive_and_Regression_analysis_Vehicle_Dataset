---
title: "car"
author: "Carbone Giorgio, Scuri Gianluca, Gazzola Michele"
date: "12/11/2021"
output:
  html_document: default
  pdf_document: default
---

# Importazione librerie

```{r importazione librerie e opzioni di rendering, include=FALSE}

library(readr)
library(tibble)
library(dplyr)
library(stringr)
library(mice)
library(caret)
library(ggcorrplot)
library(GGally) #Per il ggcorr()
library(ggrepel)
library(gridExtra)
library(glmnet)
library(xgboost)
```

# Lettura Dataset

-   Vengono assegnati i fase di lettura i tipi corretti ad ogni variabile
-   Alcune variabili presentano l'unità di misura espressa nel valore, vengono quindi temporanemanente definiti come stringhe

```{r importazione}
data_path <- "https://raw.githubusercontent.com/giocoal/datasets/main/Car%20details%20v3.csv"
car <- read_csv(data_path, col_types = 'ciiiffff????i')
head(data.frame(car))
View(car)
```

# Obiettivo

L'obiettivo di analisi consiste nella definizione di un modello di regressione lineare multipla che permetta di predire il prezzo di vendita di un'automobile usata.

# Preparazione e pulizia del dataset

#### Scegliamo di non considerare i seguenti attributi

-   Scegliamo di non considerare la colonna mileage (consumi) perchè I consumi delle auto con diverso combustibile non sono comparabili perchè i combustibili hanno un diverso rapporto energia generata/unità di volume
-   L'attributo torque, in quanto calcolato a rpm diversi per ogni veicolo/marca, lo escludiamo dall'analisi

```{r rimozione colonna torque e mileage}
car <- select(car, -c(torque,mileage))
head(car)
```

#### Conversione unità di misura della variabile selling_price

Conversione dell'unità di misura del prezzo di vendita da centesimi di dollaro in dollaro

```{r da centesimo a dollaro}
car$selling_price <- car$selling_price*0.01
```

#### Introduzione di NA per l'attributo max_power

L'attributo max_power presenta dei missing value in cui però la cella non è vuota ma presenta la sola unità di misura 'bhp'

```{r from 0 to NA}
car$max_power[car$max_power == 0 | car$max_power == "bhp"] <- NA
```

#### Verifico che le unitá di misura di ogni singolo attributo siano consistenti tra loro

```{r Controllo unita di misura}
all(grepl("CC", car$engine) == !is.na(car$engine))
#viene TRUE quindi sono tutti consistenti

all((grepl("bhp", car$max_power)) == !is.na(car$max_power))
#viene TRUE quindi sono tutti consistenti
```

#### Convesione degli attributi in cui i valori sono stringhe contenenti i valori accompagnati dall'unità di misura in valori numerici privi di unità di misura

-   A tale scopo utiliziamo la funzione `parse_value` di `dplyr`

-   Elimino gli attributi contenenti i valori accoppiati alle unità di misura con la funzione `select()` di `dplyr`

```{r Eliminazione unità di misura}
car['engine_CC'] <- parse_number(car$engine)
car['max_power_bhp'] <- parse_number(car$max_power)
car <- select(car, -c('engine', 'max_power'))
```

#### Creazione di una colonna contenente la marca dell'auto

-   Uso la funzione `word` di `dplyr` per estrarre la prima parola del nome dell'auto, la quale coincide con la marca dell'auto.

-   Creo la variabile categoriale `make` che indica la marca dell'auto e la inserisco nel dataset con la funzione `add_column` della libreria `tibble`

```{r colonna marca}
car <- add_column(car, make = factor(word(car$name, 1)), .after = "name")
```

# Statistica descrittiva

## Missing Value

-   Dalla visualizzazione prodotta con l'ausilio della funzione `md.pattern` del pacchetto `mice` notiamo come:

    -   una riga presenta un solo missing value

    -   222 righe presentano tre missing value

-   Essendo le righe contenenti `NA` circa il 3% del delle unità statistiche, decidiamo di rimuoverle

```{r missing value, echo = c(3,6), results = False}
# md.pattern (del pacchetto mice), è una funzione di visualizzazione molto utile
# Permette di vedere la distribuzione di NA nel dataset
md.pattern(car, rotate.names=TRUE)
#numero di righe con almeno un NA (coincide con quanto visto in md.pattern)
# sum(!complete.cases(car))
car <- car[-which(!complete.cases(car)),]
```

## Outliers

Creiamo un subset del dataset contenente le sole variabili quantitative

```{r Estrazione delle variabili numeriche}
car_nums_colnames <- unlist(lapply(car, is.numeric))
car_num <- car[ , car_nums_colnames]
```

Osserviamo i valori minimi e massimi delle variabili quantitative

```{r}
summary(car_num)
```

Osserviamo i boxplot

```{r boxplot}
par(mfrow=c(2,3))
for (i in names(car_num)) {
  boxplot(car_num[[i]], main = '', xlab = i)
}
par(mfrow=c(1,1))


```

```{r boxplot log}
par(mfrow=c(2,3))
for (i in names(car_num)) {
  boxplot(log(car_num[[i]]), main = '', xlab = i)
}
par(mfrow=c(1,1))

```

### Fase esplorativa

```{r lettura}
#str(car, give.attr = F)
summary(car)
#class(car)
#dim(car)

```

-   Il dataset ha 8123 unità statistiche e 13 attributi
-   Individuiamo come variabile target la variabile quantitativa continua `selling_price`

### Analisi Distribuzionale

#### Variabile Target: selling_price

Osserviamo la distribuzione della variabile target `selling_price`

```{r istogramma selling price}
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))
```

La distribuzione presenta una forte asimmetria positiva, con una coda di destra molto lunga.

Proviamo a visualizzare nuovamente la variabile dopo l'applicazione di una trasformazione logaritmica.

```{r istogramma selling_price_log}
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))+scale_x_log10()
```

La variabile target, in seguito alla trasformazione logaritmica, risulta più simmetrica, quindi questa sarà trasformata prima di procedere con la fase di inferenza e predizione.

#### Analisi distribuzione variabili esplicative numeriche

# istogrammi e box plot

```{r istogrammi, include = False}
#istogrammi 
#par(mfrow=c(2,3))
#for (i in names(car_num)) {
#  hist(car_num[[i]], xlab = i, main = '')
#}
#par(mfrow=c(1,1))
#istogrammi

plot1 <- car_num %>% ggplot()+geom_histogram(aes(year, ..density..), bins=30)+geom_density(aes(year))
print(plot1)
plot2 <- car_num %>% ggplot()+geom_histogram(aes(km_driven, ..density..), bins=30)+geom_density(aes(km_driven))
print(plot2)
plot3 <- car_num %>% ggplot()+geom_histogram(aes(seats, ..density..), bins=)+geom_density(aes(seats))
print(plot3)
plot4 <- car_num %>% ggplot()+geom_histogram(aes(engine_CC, ..density..), bins=30)+geom_density(aes(engine_CC))
print(plot4)
plot5 <- car_num %>% ggplot()+geom_histogram(aes(max_power_bhp, ..density..), bins=30)+geom_density(aes(max_power_bhp))
print(plot5)
grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol = 2)
```

Calcoliamo l'indice di asimmetria o di *skewness*

```{r}
library(psych)
for (i in names(car_num)) {
  print(i)
  print(skew(car_num[[i]]))
}
```

-   year: asimmetria negativa
-   selling_price: asimmetria positiva, coda di destra molto lunga
-   km_driven: asimmetria positiva, coda di destra molto lunga
-   seats:
-   engine:
-   max_power(bhp): leggera asimmetria positiva

Provo a valutare la distribuzione logaritmica di alcune delle variabili numeriche asimmetriche

```{r}
library(psych)
for (i in names(car_num)) {
  print(i)
  print(skew(log(car_num[[i]])))
}

```

```{r istogrammi log, include = False}
#istogrammi 
#par(mfrow=c(2,3))
#for (i in c('year','selling_price','km_driven','engine(CC)','max_power(bhp)')) {
#  hist(log(car_num[[i]]), xlab = i, main = '')
#}
#par(mfrow=c(1,1))

plot1 <- car_num %>% ggplot()+geom_histogram(aes(log(year), ..density..), bins=30)+geom_density(aes(log(year)))
plot2 <- car_num %>% ggplot()+geom_histogram(aes(log(km_driven), ..density..), bins=30)+geom_density(aes(log(km_driven)))
plot3 <- car_num %>% ggplot()+geom_histogram(aes(log(seats), ..density..), bins=)+geom_density(aes(log(seats)))
plot4 <- car_num %>% ggplot()+geom_histogram(aes(log(engine_CC), ..density..), bins=30)+geom_density(aes(log(engine_CC)))
plot5 <- car_num %>% ggplot()+geom_histogram(aes(log(max_power_bhp), ..density..), bins=30)+geom_density(aes(log(max_power_bhp)))
grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol = 2)
```

Tutte le variabili numeriche considerate, tranne yaer, sembrano assumere una distribuzione più simmetrica se applicata la trasformazione logaritmica

```{r trasformazione logaritmica}
'year'
'selling_price'
'km_driven'
'engine(CC)'
'max_power(bhp)'
```

### Valuto la presenza di outliers

```{r outliers, include = False}
#Boxplot 
par(mfrow=c(2,3))
#summary(car_num)
for (i in names(car_num)) {
  boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
```

```{r boxplot log, include = False}
#istogrammi 
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
  boxplot(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
```

#### Correlazioni

Visualizzo possibili correlazioni osservando i *pair plots* tra le variabil quantitative.\
Al fine di rendere più agile la rappresentazione grafica viene selezionato tramite *simple random sampling* un campione di 500 unità statistiche del dataset originale.

```{r correlazioni}
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]

# Matrice dei diagrammi di dispersione:
pairs(leggero[, car_nums_colnames])
cor(car[ ,car_nums_colnames])
```

Al fine di evidenziare le correlazioni osservo i *coefficienti di correlazione di Pearson*

```{r Pearson}
ggcorr(car_num, label = TRUE, label_size = 2.9, hjust = 1, layout.exp = 2)

```

# Statistica inferenziale

### Variabili dummy

```{r da variabili fattoriali a variabili dummy}

car_dummy <- car_num

# Aggiungo la variabile fattoriale 'transmission' a car dummy e la converto in numerica (0 corrisponde a 'automatic'):
car_dummy <- cbind(car_dummy,transmission = car$transmission)
levels(car_dummy$transmission)<-c(1,0)
car_dummy$transmission <- as.numeric(levels(car_dummy$transmission))[car_dummy$transmission]

#converto la variabile fattoriale 'make' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~make, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)

#converto la variabile fattoriale 'fuel' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~fuel, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)

#converto la variabile fattoriale 'seller_type' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~seller_type, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)

#converto la variabile fattoriale 'owner' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~owner, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)

head(car_dummy)
```

### Divido il dataset in Training e Test sets

```{r split dataset in Training e Test sets}
set.seed(100)
train_ind<-sample(1:nrow(car_dummy),0.8*nrow(car_dummy))

train_set<-car_dummy[train_ind,]
test_set <-car_dummy[-train_ind,]
```

\#applico un'analisi di regressione e aggiungo un numero di colonne scelte a caso che poi andranno eliminate e non valutate per l'analisi solo per poter utilizzare il comando vif per lo studio della multicollinearità. E' giusto???  #### car_dummy non va unito al dataset originale con le colonne cambiate in variabili dummy?

```{r analisi di regressione del modello di training}
car1<-train_set[,c(1:6,16:23,24:30)]
test_lm<-lm(log(selling_price) ~.,data = car1)
summary(test_lm)
(summary(test_lm)$coefficient)
```

\#applico un plot per analizzare la variabile target sui residui \#(bisogna allargare l'asse x su alcuni plot per vedere meglio l'andamento) \#Nel secondo diagramma per la regressione ideale i punti dovrebbero trovarsi sulla retta diagonale e per questo motivo il nostro risultato non è così male da rifiutare il modello. \#negli altri grafici invece affinchè si ottenga una regressione lineare corretta le linee rosse si sarebbero dovute essere parallele all'asse x (dire se i risultati sono più o meno accettabili dopo aver allargato l'asse delle x per l'osservazione)

```{r faccio un plot per analizzare i grafici che ottengo}
plot(test_lm)

```

#test della multicollinearità 

```{r test della multicollinearità}
library(car)
data.frame(vif(test_lm))

```

#Secondo la regola del valore VIF, un valore di VIF maggiore di 10 sta ad indicare che la multicollinearità può costare qualche problema al dataset. Nella nostra analisi possiamo scoprire che nessuna variabile supera nemmeno di poco il valore 10. Tutte le variabili prese in considerazione infatti non mostrano alcun problema significativo. Dalla formula di VIF sappiamo che il valore minimo di VIF è 1 e per questo set di dati la maggior parte dei valori è vicina a 1.


####### controllare se è corretto ############


#Creo una variabile cars_compr in cui permetto al modello di comprendere qual è il training set che sto utilizzando, qual è il predittore e quale variabile stiamo cercando di prevedere. Normalizzo tutti i predittore cosi da avere deviazione standard 1 e media 0. Infine creo sia un'altra variabile cars_specif che permetta al modello di sapere quale modello di regressione stiamo utilizzando, applico un vfold_cv cosi da separare i dati in 5 gruppi di ugual misura e un workflow dove racchiudo tutti i passaggi detti fino ad ora

```{r creazione variabile per comprendere qual è il predittore e cosa vogliamo prevedere e creazione finale di un workflow da inserire i dati}
library(tidyverse) # metapackage of all tidyverse packages
install.packages("tidymodels")
library(tidymodels)
library(gridExtra)

cars_compr <- recipe(selling_price~year, 
                   data=train_set)%>%
    step_normalize(all_predictors())
cars_specif <- nearest_neighbor(weight_func="rectangular", neighbors=tune()) %>%
    set_engine("kknn")%>%
    set_mode("regression")
cars_vfold <- vfold_cv(train_set, v=5, strata=selling_price)

cars_workflow <- workflow()%>%
    add_recipe(cars_compr)%>%
    add_model(cars_specif)
cars_workflow

```

#installare pacchetto kknn per SML
#Successivamente, creo un tibble di valori neighbords scelti in un range da 300 a 600 con una step_function di +3 e successivamente utilizzo le funzioni tune_grid() e collect_metrics() per ottenere le medie di un certo numero di neighbords. Eseguo questo passaggio per ottenere il numero di neighbords che mi da il valore minimo dell'RMD (root mean square).
```{r creazione di una tibble di valori neighbords per estrarre il valore con l'RDM più basso}
grid_vals <- tibble(neighbors=seq(from=300,to=600, by = 3))

cars_results <- cars_workflow%>%
    tune_grid(resamples=cars_vfold, grid=grid_vals) %>%
    collect_metrics()%>%
    filter(.metric=="rmse")
head(cars_results)

##### sistemare


```













































