hist(car_num[[i]], main = colnames(car_num)[i])
}
colnames(car_num)[1]
colnames(car_num)[year]
colnames(car_num)['year']
#names(car_num)
#car_num %>% gather() %>% head()
colnames(car_num)
for (i in names(car_num)) {
hist(car_num[[i]], main = i)
}
for (i in names(car_num)) {
hist(log(car_num[[i]]), main = i)
}
par(mfrow=(2,3))
par(mfrow=(2,3))
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]]), main = i)
#istogrammi
par(mfrow=c(3,3))
for (i in names(car_num)) {
hist(car_num[[i]]), main = i)
#istogrammi
par(mfrow=c(2,5))
for (i in names(car_num)) {
hist(car_num[[i]]), main = i)
#istogrammi
par(mfrow=c(2,5))
for (i in names(car_num)) {
hist(car_num[[i]]))
#istogrammi
par(mfrow=c(2,5))
for (i in names(car_num)) {
hist(car_num[[i]])
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]])
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i)
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
par(mfrow=c(2,3))
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
#par(mfrow=c(2,3))
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
#par(mfrow=c(2,3))
summary(car_num)
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
car$selling_price <- car$selling_price*0.01
# Estrazione delle variabili numeriche:
car_nums <- unlist(lapply(car, is.numeric))
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]
# Matrice dei diagrammi di dispersione:
pairs(leggero[, car_nums])
cor(car[ ,car_nums])
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
#par(mfrow=c(2,3))
summary(car_num)
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
rm(list=ls())
library(readr)
library(tibble)
library(dplyr)
library(stringr)
library(mice)
#library(caret)
#library(ggcorrplot)
#library(ggrepel)
#library(gridExtra)
#library(glmnet)
#library(xgboost)
#library(printr)
#library(lemon)
#knit_print.data.frame <- lemon_print
data_path <- "https://raw.githubusercontent.com/giocoal/datasets/main/Car%20details%20v3.csv"
car <- read_csv(data_path, col_types = 'ciiiffff????i')
head(data.frame(car))
View(car)
#is_tibble(car) Verifico che il ds sia di tipo tibble
str(car, give.attr = F)
summary(car)
car <- select(car, -c(torque,mileage))
head(car)
car$selling_price <- car$selling_price*0.01
car$max_power[car$max_power == "bhp"] <- NA
#car$max_power[car$max_power == 0 | car$max_power == "bhp"] <- NA
#car$mileage[car$mileage == "0.0 kmpl"] <- NA
#all(grepl("kmpl", car$mileage) == !is.na(car$mileage))
#Viene FALSE perché:
#le macchine a Gas di petrolio liquefatto (LPG) e quelle a Metano (CNG) misurano il mileage in km/kg
all(grepl("CC", car$engine) == !is.na(car$engine))
#viene TRUE quindi sono tutti consistenti
all((grepl("bhp", car$max_power)) == !is.na(car$max_power))
#viene TRUE quindi sono tutti consistenti
# Il numero di auto valore di mileage in kmpl è solo di 88 (contro 7802)
#sum(grepl("kmpl", car$mileage))
#sum(grepl("km/kg", car$mileage))
# Valore coerente con il numero totale di auto a LPG e CNG (95, probabilmente la differenza sono na)
#cat("Numero di auto LPG o CNG:", sum(table(car$fuel)[c(3, 4)]))
# Propongo di non considerare le auto a gpl e cng, dato il numero esiguo di unità statistiche
# e un possibile problema correlato al fatto che queste auto non siano sufficientemente rappresentate
#car <- subset(car, fuel != "LPG" & fuel != "CNG")
# Creo un nuovo attributo dei soli valori privi di unità di misura
# includo l'unità di misura nel nome della variabile
# La funzione parse_number mantiene i NA tali e converte tutte quelle
# celle prive di numeri in NA
#car['mileage(kmpl)'] <- parse_number(car$mileage)
car['engine(CC)'] <- parse_number(car$engine)
car['max_power(bhp)'] <- parse_number(car$max_power)
# Elimino le colonne con unità di misura usando la funzione select() di dplyr
#car <- select(car, -c('mileage', 'engine', 'max_power'))
car <- select(car, -c('engine', 'max_power'))
# Alternativa è creare una colonna di fattori 0 e 1 che indicano l'unità di misura
# Ma mi sembra un po' eccessivo dato il numero esiguo di unità statistiche
# Uso la funzione word della libreria dplyr
# la funzione add_column di tibble peraggiunge la nuova colonna dopo la colonna name
car <- add_column(car, make = factor(word(car$name, 1)), .after = "name")
# md.pattern (del pacchetto mice), è una funzione di visualizzazione molto utile
# Permette di vedere la distribuzione di NA
# nonchè di classificare le righe in funzione del numero di NA che contengono
md.pattern(car, rotate.names=TRUE)
#numero di righe con almeno un NA (coincide con quanto visto in md.pattern)
sum(!complete.cases(car))
#Trattandosi di 222 righe possiamo pensare di rimuoverle dato che costituiscono circa il 3% del dataset
car <- car[-which(!complete.cases(car)),]
class(car)
dim(car)
str(car)
summary(car)
# Estrazione delle variabili numeriche:
car_nums <- unlist(lapply(car, is.numeric))
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]
# Matrice dei diagrammi di dispersione:
pairs(leggero[, car_nums])
cor(car[ ,car_nums])
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i, main = '')
}
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_nums)) {
hist(car_nums[[i]], xlab = i, main = '')
}
# Estrazione delle variabili numeriche:
car_nums <- unlist(lapply(car, is.numeric))
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]
# Matrice dei diagrammi di dispersione:
pairs(leggero[, car_nums])
cor(car[ ,car_nums])
# Estrazione delle variabili numeriche:
car_nums <- unlist(lapply(car, is.numeric))
car_nums_dataset <- select(car, car_nums)
# Estrazione delle variabili numeriche:
car_nums <- unlist(lapply(car, is.numeric))
car_nums_dataset <- select(car, names(car) == car_nums)
# Estrazione delle variabili numeriche:
car_nums_colnames <- unlist(lapply(car, is.numeric))
car_num <- car[ , car_nums_colnames]
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]
# Matrice dei diagrammi di dispersione:
pairs(leggero[, car_nums_colnames])
cor(car[ ,car_nums_colnames])
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_nums[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
#par(mfrow=c(2,3))
summary(car_num)
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
#par(mfrow=c(2,3))
summary(car_num)
#for (i in names(car_num)) {
#  boxplot(car_num[[i]], xlab = i, main = '')
#}
#par(mfrow=c(1,1))
view(car)
#istogrammi
par(mfrow=c(2,3))
for (i in names(car_num)) {
hist(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#Boxplot
par(mfrow=c(2,3))
#summary(car_num)
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
boxplot(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,2))
for (i in c('year','selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
#istogrammi
par(mfrow=c(2,3))
for (i in c('year','selling_price','km_driven','engine(CC)','max_power(bhp)')) {
hist(log(car_num[[i]]), xlab = i, main = '')
}
par(mfrow=c(1,1))
library(readr)
library(tibble)
library(dplyr)
library(stringr)
library(readr)
library(tibble)
library(dplyr)
library(stringr)
car1<-train_set[,c(1:6,16:23,24:30)]
car_dummy <- car_num
# Aggiungo la variabile fattoriale 'transmission' a car dummy e la converto in numerica (0 corrisponde a 'automatic'):
car_dummy <- cbind(car_dummy,transmission = car$transmission)
levels(car_dummy$transmission)<-c(1,0)
car_dummy$transmission <- as.numeric(levels(car_dummy$transmission))[car_dummy$transmission]
#converto la variabile fattoriale 'make' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~make, data = car))[,-1]
#converto la variabile fattoriale 'fuel' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~fuel, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)
#converto la variabile fattoriale 'seller_type' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~seller_type, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)
#converto la variabile fattoriale 'owner' in dummy e la aggiungo a car_dummy:
dummy_temp <- data.frame(model.matrix( ~owner, data = car))[,-1]
car_dummy <-cbind(car_dummy,dummy_temp)
car_dummy <- car_num
library(readr)
library(tibble)
library(dplyr)
library(stringr)
library(mice)
library(mice)
library(caret)
library(gridExtra)
library(glmnet)
library(xgboost)
# Lettura Dataset
-   Vengono assegnati i fase di lettura i tipi corretti ad ogni variabile
-   Alcune variabili presentano l'unità di misura espressa nel valore, vengono quindi temporanemanente definiti come stringhe
```{r importazione}
data_path <- "https://raw.githubusercontent.com/giocoal/datasets/main/Car%20details%20v3.csv"
car <- read_csv(data_path, col_types = 'ciiiffff????i')
# Obiettivo
L'obiettivo di analisi consiste nella definizione di un modello di regressione lineare multipla che permetta di predire il prezzo di vendita di un'automobile usata.
# Preparazione e pulizia del dataset
#### Scegliamo di non considerare i seguenti attributi
-   Scegliamo di non considerare la colonna mileage (consumi) perchè I consumi delle auto con diverso combustibile non sono comparabili perchè i combustibili hanno un diverso rapporto energia generata/unità di volume
-   L'attributo torque, in quanto calcolato a rpm diversi per ogni veicolo/marca, lo escludiamo dall'analisi
```{r rimozione colonna torque e mileage}
car <- select(car, -c(torque,mileage))
head(car)
car <- select(car, -c(torque,mileage))
#### Conversione unità di misura della variabile selling_price
Conversione dell'unità di misura del prezzo di vendita da centesimi di dollaro in dollaro
```{r da centesimo a dollaro}
car$selling_price <- car$selling_price*0.01
car$selling_price <- car$selling_price*0.01
#### Introduzione di NA per l'attributo max_power
L'attributo max_power presenta dei missing value in cui però la cella non è vuota ma presenta la sola unità di misura 'bhp'
```{r from 0 to NA}
car$max_power[car$max_power == 0 | car$max_power == "bhp"] <- NA
car$max_power[car$max_power == 0 | car$max_power == "bhp"] <- NA
#### Verifico che le unitá di misura di ogni singolo attributo siano consistenti tra loro
```{r Controllo unita di misura}
all(grepl("CC", car$engine) == !is.na(car$engine))
all((grepl("bhp", car$max_power)) == !is.na(car$max_power))
#viene TRUE quindi sono tutti consistenti
#### Convesione degli attributi in cui i valori sono stringhe contenenti i valori accompagnati dall'unità di misura in valori numerici privi di unità di misura
-   A tale scopo utiliziamo la funzione `parse_value` di `dplyr`
-   Elimino gli attributi contenenti i valori accoppiati alle unità di misura con la funzione `select()` di `dplyr`
```{r Eliminazione unità di misura}
car['engine_CC'] <- parse_number(car$engine)
car['max_power_bhp'] <- parse_number(car$max_power)
car <- select(car, -c('engine', 'max_power'))
car['engine_CC'] <- parse_number(car$engine)
#### Creazione di una colonna contenente la marca dell'auto
-   Uso la funzione `word` di `dplyr` per estrarre la prima parola del nome dell'auto, la quale coincide con la marca dell'auto.
-   Creo la variabile categoriale `make` che indica la marca dell'auto e la inserisco nel dataset con la funzione `add_column` della libreria `tibble`
```{r colonna marca}
car <- add_column(car, make = factor(word(car$name, 1)), .after = "name")
car <- add_column(car, make = factor(word(car$name, 1)), .after = "name")
# Statistica descrittiva
## Missing Value
-   Dalla visualizzazione prodotta con l'ausilio della funzione `md.pattern` del pacchetto `mice` notiamo come:
-   una riga presenta un solo missing value
-   222 righe presentano tre missing value
-   Essendo le righe contenenti `NA` circa il 3% del delle unità statistiche, decidiamo di rimuoverle
```{r missing value, echo = c(3,6), results = False}
# md.pattern (del pacchetto mice), è una funzione di visualizzazione molto utile
# Permette di vedere la distribuzione di NA nel dataset
md.pattern(car, rotate.names=TRUE)
## Outliers
Creiamo un subset del dataset contenente le sole variabili quantitative
Osserviamo i valori minimi e massimi delle variabili quantitative
Osserviamo i boxplot
### Fase esplorativa
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))
La distribuzione presenta una forte asimmetria positiva, con una coda di destra molto lunga.
Proviamo a visualizzare nuovamente la variabile dopo l'applicazione di una trasformazione logaritmica.
```{r istogramma selling_price_log}
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))+scale_x_log10()
car %>% ggplot()+geom_histogram(aes(selling_price, ..density..), bins=50)+geom_density(aes(selling_price))+scale_x_log10()
La variabile target, in seguito alla trasformazione logaritmica, risulta più simmetrica, quindi questa sarà trasformata prima di procedere con la fase di inferenza e predizione.
#### Analisi distribuzione variabili esplicative numeriche
# istogrammi e box plot
print(plot1)
print(plot2)
print(plot3)
print(plot4)
plot5 <- car_num %>% ggplot()+geom_histogram(aes(max_power_bhp, ..density..), bins=30)+geom_density(aes(max_power_bhp))
print(plot5)
plot1 <- car_num %>% ggplot()+geom_histogram(aes(year, ..density..), bins=30)+geom_density(aes(year))
Calcoliamo l'indice di asimmetria o di *skewness*
```{r}
library(psych)
library(psych)
for (i in names(car_num)) {
print(i)
print(skew(car_num[[i]]))
}
-   year: asimmetria negativa
-   selling_price: asimmetria positiva, coda di destra molto lunga
-   km_driven: asimmetria positiva, coda di destra molto lunga
-   seats:
-   engine:
-   max_power(bhp): leggera asimmetria positiva
Provo a valutare la distribuzione logaritmica di alcune delle variabili numeriche asimmetriche
```{r}
for (i in names(car_num)) {
print(i)
print(skew(log(car_num[[i]])))
}
plot1 <- car_num %>% ggplot()+geom_histogram(aes(log(year), ..density..), bins=30)+geom_density(aes(log(year)))
plot3 <- car_num %>% ggplot()+geom_histogram(aes(log(seats), ..density..), bins=)+geom_density(aes(log(seats)))
plot5 <- car_num %>% ggplot()+geom_histogram(aes(log(max_power_bhp), ..density..), bins=30)+geom_density(aes(log(max_power_bhp)))
plot1 <- car_num %>% ggplot()+geom_histogram(aes(log(year), ..density..), bins=30)+geom_density(aes(log(year)))
Tutte le variabili numeriche considerate, tranne yaer, sembrano assumere una distribuzione più simmetrica se applicata la trasformazione logaritmica
```{r trasformazione logaritmica}
'selling_price'
'km_driven'
'max_power(bhp)'
'year'
'selling_price'
'km_driven'
'engine(CC)'
'max_power(bhp)'
### Valuto la presenza di outliers
```{r outliers, include = False}
par(mfrow=c(1,1))
#Boxplot
par(mfrow=c(2,3))
#summary(car_num)
for (i in names(car_num)) {
boxplot(car_num[[i]], xlab = i, main = '')
}
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
boxplot(log(car_num[[i]]), xlab = i, main = '')
}
#istogrammi
par(mfrow=c(2,2))
for (i in c('selling_price','km_driven','engine(CC)','max_power(bhp)')) {
boxplot(log(car_num[[i]]), xlab = i, main = '')
}
#### Correlazioni
Visualizzo possibili correlazioni osservando i *pair plots* tra le variabil quantitative.\
Al fine di rendere più agile la rappresentazione grafica viene selezionato tramite *simple random sampling* un campione di 500 unità statistiche del dataset originale.
```{r correlazioni}
# Prendo un campione di osservazioni per rendere più agile la rappresentazione grafica:
leggero <- car[sample(nrow(car), 500), ]
cor(car[ ,car_nums_colnames])
Al fine di evidenziare le correlazioni osservo i *coefficienti di correlazione di Pearson*
```{r Pearson}
# Statistica inferenziale
### Variabili dummy
```{r da variabili fattoriali a variabili dummy}
# Aggiungo la variabile fattoriale 'transmission' a car dummy e la converto in numerica (0 corrisponde a 'automatic'):
car_dummy <- cbind(car_dummy,transmission = car$transmission)
car_dummy$transmission <- as.numeric(levels(car_dummy$transmission))[car_dummy$transmission]
car_dummy <-cbind(car_dummy,dummy_temp)
car_dummy <-cbind(car_dummy,dummy_temp)
car_dummy <-cbind(car_dummy,dummy_temp)
car_dummy <-cbind(car_dummy,dummy_temp)
head(car_dummy)
### Divido il dataset in Training e Test sets
```{r split dataset in Training e Test sets}
set.seed(100)
train_ind<-sample(1:nrow(car_dummy),0.8*nrow(car_dummy))
test_set <-car_dummy[-train_ind,]
\#applico un'analisi di regressione e aggiungo un numero di colonne scelte a caso che poi andranno eliminate e non valutate per l'analisi solo per poter utilizzare il comando vif per lo studio della multicollinearità. E' giusto???  #### car_dummy non va unito al dataset originale con le colonne cambiate in variabili dummy?
```{r analisi di regressione del modello di training}
test_lm<-lm(log(selling_price) ~.,data = car1)
(summary(test_lm)$coefficient)
\#applico un plot per analizzare la variabile target sui residui \#(bisogna allargare l'asse x su alcuni plot per vedere meglio l'andamento) \#Nel secondo diagramma per la regressione ideale i punti dovrebbero trovarsi sulla retta diagonale e per questo motivo il nostro risultato non è così male da rifiutare il modello. \#negli altri grafici invece affinchè si ottenga una regressione lineare corretta le linee rosse si sarebbero dovute essere parallele all'asse x (dire se i risultati sono più o meno accettabili dopo aver allargato l'asse delle x per l'osservazione)
```{r faccio un plot per analizzare i grafici che ottengo}
plot(test_lm)
#test della multicollinearità
```{r test della multicollinearità}
data.frame(vif(test_lm))
#Secondo la regola del valore VIF, un valore di VIF maggiore di 10 sta ad indicare che la multicollinearità può costare qualche problema al dataset. Nella nostra analisi possiamo scoprire che nessuna variabile supera nemmeno di poco il valore 10. Tutte le variabili prese in considerazione infatti non mostrano alcun problema significativo. Dalla formula di VIF sappiamo che il valore minimo di VIF è 1 e per questo set di dati la maggior parte dei valori è vicina a 1.
####### controllare se è corretto ############
#Creo una variabile cars_compr in cui permetto al modello di comprendere qual è il training set che sto utilizzando, qual è il predittore e quale variabile stiamo cercando di prevedere. Normalizzo tutti i predittore cosi da avere deviazione standard 1 e media 0. Infine creo sia un'altra variabile cars_specif che permetta al modello di sapere quale modello di regressione stiamo utilizzando, applico un vfold_cv cosi da separare i dati in 5 gruppi di ugual misura e un workflow dove racchiudo tutti i passaggi detti fino ad ora
```{r creazione variabile per comprendere qual è il predittore e cosa vogliamo prevedere e creazione finale di un workflow da inserire i dati}
install.packages("tidymodels")
#installare pacchetto kknn per SML
#Successivamente, creo un tibble di valori neighbords scelti in un range da 300 a 600 con una step_function di +3 e successivamente utilizzo le funzioni tune_grid() e collect_metrics() per ottenere le medie di un certo numero di neighbords. Eseguo questo passaggio per ottenere il numero di neighbords che mi da il valore minimo dell'RMD (root mean square).
```{r creazione di una tibble di valori neighbords per estrarre il valore con l'RDM più basso}
grid_vals <- tibble(neighbors=seq(from=300,to=600, by = 3))
test_set <-car_dummy[-train_ind,]
car_dummy <- car_num
grid_vals <- tibble(neighbors=seq(from=300,to=600, by = 3))
cars_results <- cars_workflow%>%
tune_grid(resamples=cars_vfold, grid=grid_vals) %>%
collect_metrics()%>%
filter(.metric=="rmse")
cars_results <- cars_workflow%>%
tune_grid(resamples=cars_vfold, grid=grid_vals) %>%
collect_metrics()%>%
filter(.metric=="rmse")
library(tidyverse) # metapackage of all tidyverse packages
library(tidymodels)
library(gridExtra)
cars_compr <- recipe(selling_price~year,
data=train_set)%>%
step_normalize(all_predictors())
cars_specif <- nearest_neighbor(weight_func="rectangular", neighbors=tune()) %>%
set_engine("kknn")%>%
set_mode("regression")
cars_vfold <- vfold_cv(train_set, v=5, strata=selling_price)
set.seed(100)
train_ind<-sample(1:nrow(car_dummy),0.8*nrow(car_dummy))
car_dummy <- car_num
